\documentclass[a4paper,12pt]{scrartcl}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{mathbbol}

\usepackage{graphicx} % Include figure files
\usepackage[margin=10pt,font=small,labelfont=bf,labelsep=colon, format = plain, indention = .5cm]{caption} %sch√∂ne Untertitel

\renewcommand{\vec}[1]{{\mathbf{#1}}}
%opening
\title{Discontinuous Galerkin Methods}
\author{Matthias  Wiesenberger}

% in vim write a word and press F5 to get a begin-end structure (or \usepackage in the header)
% then use STRG-J to jump to the next field
% press two times ` and / to get a \frac{}{} environment
\begin{document}

\maketitle

\begin{abstract}
In this report we review the discontinuous Galerkin methods used to discretize the toefl equations. 
We first give a short introduction and then derive 
a discretization for the first derivative of a 1D function. 
This is then used to construct
a new 'Arakawa-like' scheme used to discretize Poisson's bracket. 
We present the numerically found orders and conservation properties.
For the Laplacian we basically follow the existing literature and present 
the method to solve Poisson's equation with periodic and dirichlet boundary
conditions in 1D and 2D. 
Combined with a Runge-Kutta method to discretize time we then analyze the 
numerical conservation of vorticity, enstrophy and energy of the 
so-called Lamb dipole. 
\end{abstract}

\section{Introduction}
The name discontinuous Galerkin (DG) encompasses a whole family of 
discretization methods for the spatial part of differential equations. 
The main idea of a Discontinuous Galerkin method is to approximate the solution 
of a differential equation by a polynomial in each grid-cell. 
In classical finite element methods continuity is required across cell boundaries. 
DG methods in contrast allow discontinuities across cell boundaries. 
A major benefit of DG above other discretization methods is their high parallelizability,
while retaining high order at the same time. This is because the resulting 
 stencils remain local. Moreover, in the same framework, higher/lower
order methods can be constructed by simply increasing/decreasing the degree of 
the polynomials used in each cell. 

We will use the so-called local discontinuous Galerkin method (LDG) in order 
to discretize the Laplacian. Together with a specially constructed Runge-Kutta 
scheme for time-discretization this will lead to the Runge-Kutta discontinuous galerkin methods (RKDG). 
The latter methods
have developped hand in hand with the flux-conservative and slope-limiter 
concepts in finite volume methods, and thus are well - suited for 
conservation equations and can be easily made shock-capturing. 
This hopefully balances the disadvantage of multiple right-hand-side
evaluations above multistep or multivalue algorithms.
Local discontinuous Galerkin methods were actually introduced by [ Cockburn and Shu, 1998] for the discretisation of parabolic equations, but
it was later found that the method and its advantages could also be used for the solution of elliptic 
equations (namely Poisson's equation). 
[Arnold et al., J. NUMER. ANAL., 2002] highlighted the relation 
of this method to interior penalty and other alternative methods. 
[Cockburn et al., J.NUMER. ANAL. ,2001] proved a superconvergence result for the 
local discontinuous Galerkin approach in cartesian grids, which makes the method seem quite attractive for our purposes.

In a first approach we use an explicit (3rd order) Runge-Kutta method for 
the time discretization, but we might want to try higher order methods as well.   
An excellent review of the Runge-Kutta discontinuous Galerkin methods can be found in [Cockburn et al., Journal of scientific Comp., 2001]. 

We expect the method to suffer from a CFL condition, (that is unfortunately more severe, the 
higher the order of the method) but we expect that in increasing the order of 
our base polynomials we might be able to use a coarser grid to achieve the same resolution 
and thus increase the time step as well. 




\section{ Discretization of derivatives}
We begin to derive the first derivative of a one-dimensional function.
For simplicity we choose an equidistant grid with $N$ cells $K_n$ and gridsize $h$.
With this choice we are able to choose basis functions of $P(K_n)$, the space of 
polynomials of degree at most $P-1$ on $K_n$, via orthogonal Legendre polynomials. 

Before we dive right into things it seems justified to take a closer look at the 
properties of the Legendre polynomials and to define some useful quantities.


\subsection{ The Legendre polynomials}
Legendre polynomials can be recursively defined on $[-1,1]$ by setting
$p_0(x) = 1$, $p_1(x) = x$ and
\begin{align}
    (n+1)p_{n+1}(x) = (2n+1)xp_n(x) - np_{n-1}(x)
    \label{eq:recursion}
\end{align}
This results in a set of polynomials $p_k(x)$ of degree $k$ which are orthogonal
\begin{align}
    \int_{-1}^{1} p_m(x)p_n(x) dx = \frac{2}{2n+1}\delta_{mn} 
    \label{eq:orthogonal}
\end{align}
and complete in $L^2([-1,1])$ with the standard $L^2$-norm. We thus have
\begin{align}
    f(x) = \sum_{n=0}^\infty c_n p_n(x)
    \label{eq:expansion}
\end{align} with
\begin{align}
    c_n = \frac{2n+1}{2}\int_{-1}^{1} f(x)p_n(x)dx
    \label{eq:coefficient}
\end{align}
We also note that $\forall n$ and $x\in[-1,1]$
\begin{subequations}
\begin{align}
    p_n(1) = 1 \\
    p_n(-x) = (-1)^np_n(x)
\end{align}
\label{eq:boundaries}
\end{subequations}
In the finite dimensional case we write 
$x_j$ and $w_j$, $j=0,\dots,P-1$, as abscissas and weights of 
Gauss-Legendre integration and note
\begin{align}
    S_{kl} := \int_{-1}^1 p_k(x)p_l(x) dx = \sum_{j=0}^{P-1} w_jp_k (x_j)p_l(x_j) = \frac{2}{2k+1}\delta_{kl} =: S_k \delta_{kl}
    \label{}
\end{align}
since Gauss-Legndre integration is exact for polynomials of degree at most $2P-1$.
The discrete completeness relation reads
\begin{align}
    \sum_{k=0}^{P-1} \frac{2k+1}{2}w_j p_k(x_i)p_k(x_j) = \delta_{ij} 
    \label{}
\end{align}
We thus have for $f_j = f(x_j)$
\begin{subequations}
\begin{align}
    c_k = \frac{2k+1}{2}\sum_{j=0}^{P-1}w_j p_k(x_j) f_j =: \sum_{j=0}^{P-1} F_{kj}f_j\\
    f_j = \sum_{k=0}^{P-1} p_k(x_j) c_k =: \sum_{j=0}^{P-1} B_{jk}c_k
\end{align}
\end{subequations}
Note that the use of Legendre polynomials yields a natural approximation of the integrals of $f$
via Gauss-Legendre quadrature:
\begin{align}
   \int_{-1}^1 f(x) dx &\approx  \frac{1}{2}\sum_{j=0}^{P-1} w_j f_j = c_0 \\
   \int_{-1}^1 |f|^2 dx &\approx \sum_{j=0}^{P-1} w_jf_j^2 = \sum_{j=0}^{P-1}s_0 F_{0j}f_j^2 = \sum_{j=0}^{P-1} s_jc_j^2  \\
   <f,g>:=\int_{-1}^1 fg dx &\approx \sum_{j,k=0}^{P-1}F_j S_{jk} G_k
    \label{}
\end{align}
In order to use Legendre polynomials in our grid cells $K_n$ centered at $x_n$
we have to transform them appropriately:
\begin{align}
    p^n_j(x) := p_j( \frac{2}{h}(x-x_n))
    \label{}
\end{align}
We define some useful quantities for $i,j=0,1,\dots,P-1$
\begin{subequations}
    \begin{align}
        S_{ij} &:= \int_{-h/2}^{h/2} p_i(\frac{2}{h} x)p_j(\frac{2}{h} x) dx = \frac{h}{2i+1}\delta_{ij} =: s_i \delta_{ij}\\ 
        T_{ij} &:= s^{-1}_{ij} = \frac{2i+1}{h}\delta_{ij} =: t_i \delta_{ij}\\
        D_{ij} &:= \int_{-h/2}^{h/2} p_i(\frac{2}{h} x)\partial_xp_j(\frac{2}{h} x) dx\\
        R_{ij} &:= p_i(1)p_j(1) = 1 = R_{ji}\\
        L_{ij} &:= p_i(-1)p_j(-1) = (-1)^{i+j} = L_{ji}\\
        RL_{ij}&:= p_i(1)p_j(-1) = (-1)^j\\
        LR_{ij}&:= p_i(-1)p_j(1) = (-1)^i = RL^T_{ij}
        \label{eq:legendre_operators}
    \end{align}
\end{subequations}
In order to get the elements of $D_{ij}$ we first note that $D_{ij} = 0$ for
$i > j-1$ because $\partial_x p_j(x)$ is a polynomial of degree $j-1$. Then
we use the integration by parts formula to get
\begin{align}
    (D+L) = (R-D)^T
    \label{eq:legendre_derivative}
\end{align}
i.e. $D_{ij} = 1 - (-1)^{i+j}$ for $i\le (j-1)$. We thus have e.g. for $P=4$
\begin{align}
    D = \begin{pmatrix}
        0 & 2 & 0 & 2 \\
        0 & 0 & 2 & 0 \\
        0 & 0 & 0 & 2 \\
        0 & 0 & 0 & 0 
    \end{pmatrix}
    \label{eq:example_derivative}
\end{align}
We thus have that $S(h)$ and $T(h)$ are the only projection matrices that dependent on the gridsize $h$. The discontinuous Galerkin expansion of a function $f$ in the intervall $[a,b]$ can 
then readily be given as
\begin{align}
    f(x) = \sum_{n=1}^N \sum_{k=0}^{P-1} f^n_k p_{nk}(x)
\end{align}
and the integration formulas are straightforwardly extended to the whole domain.

















\subsection{ Discretization of first derivatives}
From here on we write
$ f = f^n_kp_{nk}(x)$ and imply the summation over $k$ and $n$.
We begin to expand $\partial_x f$
with Legendre polynomials and then integrate by parts. For 
simplicity we use periodic boundaries. 
\begin{align}
    (\partial_x f)^n_i = t_{ij}\int\partial_x f(x) p_j(x) dx = t_{ij}\left( [\hat f p_j]_{x_{n-1/2}}^{x_{n+1/2}} - \int f \partial_x p_j dx \right)
    \label{}
\end{align}
Note that the expansion of $f$ is double valued on cell boundaries. 
That means we have a freedom in choosing a so-called numerical flux $\hat f$.
Here we choose the average on the cell boundary
\begin{align}
    \hat f(x_{n+1/2}) \equiv \frac{1}{2}(f^{n+1}_kp_k(-1) + f^n_k p_k(1))
    \label{}
\end{align}
We thus arrive at
\begin{align}
    (\partial_x f)^n_i= t_{ij}\left(  \frac{1}{2}[(f^{n+1}_kp_k(-1)+ f^n_kp_k(1))p_j(1) 
                - (f^n_kp_k(-1) + f^{n-1}_kp_k(1))p_j(-1)] - f^n_k d_{kj} \right)
    \label{}
\end{align}
With the notation previously introduced this reduces to 
\begin{align}
    \partial_x = \frac{1}{2}T\circ \left[ \Eins^+\otimes RL + \Eins \otimes (D-D^T) - \Eins^-\otimes LR\right]
    \label{}
\end{align}
where we used that $D+D^T = R-L$. We now have formulated a DG discretization of the first derivative in the form of a matrix vector product. 
We note that in the case of $P = 1$ it reduces to the familiar classical finite
differences formula eqn(\ref{eq:derivative}).

We now numerically test the order of this derivation, by computing the derivative
of $f(x) = sin(x)$. We compute the absolute error in the $L_2$ - norm 
\begin{align}
    \lVert f\rVert = \sqrt{ <f,f>} \equiv \left( \int |f|^2 dx \right)^{1/2}
    \label{eq:norm}
\end{align}
We compute the order $p$ by computing the derivative twice with $N=10$ and $N=20$ grid
points.
\begin{align*}
    n = 1 \rightarrow p = 2 \\
    n = 2 \rightarrow p = 1 \\
    n = 3 \rightarrow p = 3 \\
    n = 4 \rightarrow p = 3 \\
    n = 5 \rightarrow p = 5
\end{align*}
We conclude that the method is of order $n$ for $n$ odd, and $n-1$ for $n$ even. 







\section{ Discretization of Poisson bracket}

We now come to the discretization of the two-dimensional Poisson bracket (Jacobian)
\begin{align}
    J(f,g) \equiv \partial_x f \partial_y g - \partial_y f\partial_x g
    \label{eq:jacobian}
\end{align}
As [Arakawa, J. Comp. Phys., 1997] showed, it is important to numerically conserve
the following three quantities
\begin{subequations}
\begin{align}
    \int_G J(f,g) dxdy = 0 \\
    \int_G fJ(f,g) dxdy = 0\\
    \int_G gJ(f,g) dxdy = 0
    \label{eq:conservation}
\end{align}
\end{subequations}
which for two-dimensional incompressible fluid flow correspond to the conservation of
total vorticity, enstrophy and kinetic energy respectively.
The main idea in [Arakawa, J. Comp. Phys., 1997] is
to equally add three different discretisations of the Jacobian called $J^{++}$, 
$J^{+x}$ and $J^{x+}$ to get a discretisaion which respects all of 
eqns (\ref{eq:conservation}) by setting
\begin{align}
    J = \frac{1}{3}\left( J^{++}+J^{x+}+J^{+x} \right)   
    \label{eq:construction}
\end{align}
The Jacobians were derived by first introducing a symmetric discretisation for the
simple derivative, namely
\begin{align}
    (\partial_x f)_{ij} = \frac{f_{i+1,j}-f_{i-1,j}}{2h} + O(h^2)
    \label{eq:derivative}
\end{align}
and 
\begin{align}
    (f\partial_x g)_{ij} = f_{ij}\frac{g_{i+1,j}-g_{i-1,j}}{2h}+O(h^2)
    \label{}
\end{align}
The Jacobians were then defined as
\begin{subequations}
\begin{align}
    J_{ij}^{++} &= (\partial_x f)_{ij} (\partial_y g)_{ij} - (\partial_y f)_{ij}(\partial_x g)_{ij} \\
    J_{ij}^{+x} &= (\partial_x( f\partial_y g))_{ij} - (\partial_y( f\partial_x g))_{ij}\\
    J_{ij}^{x+} &= (\partial_y( g\partial_x f))_{ij} - (\partial_y( g\partial_x f))_{ij}
    \label{}
\end{align}
\label{}
\end{subequations}
Note that the difference between the three formulations lies in the 
order of multiplications and derivatives. 
For example the first term in $J^{+x}$ means that we
first compute the numerical derivative of $g$ in $y$, 
then multiply the result with 
$f$ and then compute the numerical derivative of the result in $x$.
Each of these Jacobians conserves one and only one of the quantities (\ref{eq:conservation}) but when they are brought together according to eqn( \ref{eq:construction})
all quantities are conserved. 

For the DG discretization of the Jacobian we now simply follow the route set by
Arakawa. We first derive a discretization for the first derivative of a function
and for the product of a function with a derivative. We then use eqn( \ref{eq:construction}) to derive our final discretization scheme. 







\subsection{ Product of two functions }
If $f$ and $g$ are two functions represented by a DG approximation $f = f_kp_k$ and
$g = g_lp_l$ then 
\begin{align}
    (fg)_i = t_{ij}\int (fg)p_j dx = t_{ij} f_k g_l \int p_k p_l p_j dx =: t_{ij} M_{klj} f_k g_l
    \label{}
\end{align}
Note that $M$ is symmetric in all of its indices. All we need now is an expression for
$t_{ij}M_{klj}$: for this we use the Gaussian integration formula
\begin{align}
    \int p_k p_l p_j dx = \sum_i w_i p_k(x_i)p_l(x_i)p_j(x_i) = \sum_i w_i B_{ik}B_{il}B_{ij}
    \label{}
\end{align}
Recall that $F_{ij} = t_j w_i B_{ij}$ and arrive at
\begin{align}
    (fg)_i = t_{ij} M_{klj} f_k g_l = \sum_m F_{jm} B_{mk}f_k B_{ml}g_l
    \label{}
\end{align}
This result is not surprising. It simply states that we can compute the product
of two DG-expanded functions by backtransforming them to $x$-space, then performing
a pointwise multiplication and then transforming the result to $L$-space again.











\subsection{ The DG Jacobian}
We are now ready to construct a discretization for the Jacobian. 
We note 
\begin{subequations}
    \begin{align}
        J^{++}_{ij} &= ((\partial_x f)(\partial_y g))_{ij} - ((\partial_y f)(\partial_x g))_{ij} \\
        J^{+x}_{ij} &= (\partial_x ( f\partial_y g))_{ij} - (\partial_y (f\partial_x g))_{ij} \\
        J^{x+}_{ij} &= (\partial_y (g\partial_x f))_{ij} - (\partial_x( g\partial_y f))_{ij}
        \label{}
    \end{align}
    \label{}
\end{subequations}
Again the difference between the three formulations is when the multiplication
and when the derivation is performed. 
The final form of the Jacobian is then given by eq. (\ref{eq:construction}):
\begin{align}
    J = \frac{1}{3}(J^{++} + J^{+x} + J^{x+})
    \label{}
\end{align}

We test our scheme with $f(x,y) = sin(x)*cos(y)$ and 
$g(x,y) = cos(x)*sin(y)$ which yields 
$\{f,g\} = cos^2(x)*cos^2(y) - sin^2(x)*sin^2(y)$. 












\section{ Discretization of Poisson's equation}
We begin with a discretization scheme for the Laplacian in Poisson's equation.
We choose our spatial grid as the product room of two one dimensional
discretizations: $T_h := I_x \times I_y$ such that element $K_{ij} = I_i \times I_j$,
is a rectangle. With this choice we are able to choose basis functions 
of $P(K)$, the space of polynomials of degree at most $p$ on $K$, 
via simple products of orthogonal Legendre polynomials: 
$p_{ij}(x,y) := p_i(x) p_j(y)$. Poisson's equation thus can be written as
\begin{align}
    \Delta \phi = \partial_x^2 \phi + \partial_y^2 \phi = (\Eins_y \otimes \hat O_x + \hat O_y \otimes \Eins_x) \phi = \rho
    \label{eq:product_discretization}
\end{align}
where $\hat O_{x/y}$ is the DG discretization of $\partial_{x/y}^2$. This means we 
can concentrate on discretizing the 1D Poisson equation and then use
equation (\ref{eq:product_discretization}) to get a formula for the 2D Poisson 
equation.
\subsection{Discretization of the 1d Poisson equation}
We want to solve 
\begin{align}
    -\partial_x^2\phi(x) = \rho(x), \ \ \ x\in [0,1]
    \label{eq:1d_poisson}
\end{align}
for various boundary conditions specified later.
We first rewrite Poisson's equation as two first order equations.
Denote $j = -\partial_x \phi$ and write
\begin{subequations}
\begin{align}
    j + \partial_x \phi &= 0\\
    \partial_x j &= \rho 
    \label{eq:poisson}
\end{align}
\end{subequations}
We multiply by test functions $v \in V_h$ and $w \in V_h$ with 
support only in cell $I^n$ and 
integrate by parts.
\begin{subequations}
    \begin{align}
        \int_{I^n} j_h w dx &= \int_{I^n}\phi_h\partial_x w dx - [\phi_f w]_{x_{n-1/2}}^{x_{n+1/2}} \\
        \int_{I^n} \rho v dx &= -\int_{I^n} j_h\partial_x v dx + [j_f v]_{x_{n-1/2}}^{x_{n+1/2}}
    \end{align}
    \label{eq:dg_poisson}
\end{subequations}
We have to choose the fluxes $\phi_f$ and $j_f$ consistently.
We follow [Cockburn et al., J. NUMER. ANAL, 2001] and define
\begin{subequations}
\begin{align}
    \phi_f(x_{n+1/2}) &:= \phi_h^{n+1}(x_{n+1/2}) = \phi^{n+1}_kp_k(-1) \\
    j_f(x_{n+1/2}) &:= j_h^n(x_{n+1/2}) + [\phi_h] = j^n_kp_k(1) + [\phi_h] 
    \label{eq:dg_fluxes}
\end{align}
\end{subequations} 
where $[\phi_h] := \phi_h^n(x_{n+1/2})-\phi_h^{n+1}(x_{n+1/2})$ denotes
the jump of $\phi_h$. Eq. (\ref{eq:dg_fluxes}) mean that we alternatingly 
take the right boundary value of $\phi_h$ (which is the value of $\phi_h$ in 
the right cell on the left side) and 
the left boundary value of $j_h$ and add $[\phi_h]$ to $j_f$. 
Recall here that $\phi_h$ and $j_h$ are 
double valued on $x_{n+1/2}$.
On the boundary we choose
\begin{subequations}
    \begin{align}
        \phi_f(x_b) &:= \phi_b \\
        j_f(x_b) &:= j_h^i(x_b) + [\phi_h]
        \label{ eq:dg_dirichlet_boundary_fluxes}
    \end{align}
\end{subequations}
where the subscript $i$ denotes the interior cell. $[\phi_h]$ is well
defined on the boundary for Drichlet boundary conditions. If we 
choose von Neumann boundary condition we define
\begin{subequations}
    \begin{align}
        \phi_f(x_b) &:= \phi^i_h(x_b) \\
        j_f(x_b) &:= j_b
        \label{eq:dg_neumann_boundary_fluxes}
    \end{align}
\end{subequations}
For periodic boundary conditions there is nothing to define for 
the grid-topology is a circle then. 

The method now consists in finding $(j_h, \phi_h)$ such that for all 
testfunctions $v, w\in P(I_n)$, in all elements $I_n$, equations 
(\ref{eq:dg_poisson}) are satisfied.

We first take advantage of the fact that the flux $\phi_f$ doesn't 
depend on $j_h$, so we can locally eliminate the auxiliary 
variable $\vec j_h$, hence the name ``local discontinuous galerkin method``.
(The term local was originally introduced to distinguish the 
method from continuous Galerkin methods used to solve parabolic equations.)
Before doing so we will introduce a useful notation technique, namely the
tensor product for matrices.
 












\subsection{ Kronecker-Product}
Suppose we have a term of the form $m_{ij} = a_{ik}b_{jl}n_{kl}$ where 
$m$ and $n$ are equally shaped matrices and $a$ and $b$ are 
square matrices of appropriate sizes. We write this as
\begin{align}
    M = (A\otimes B) N
    \label{eq:kronecker_product}
\end{align}
i.e. $A$ operates on the first index of $N$ and $B$ operates on the 
second index. For example the discontinuous galerkin expansion of 
a function $\phi_h$ reads $\phi_h = \sum_{n,k} phi^n_{k} p_{nk}(x)$ where
$n$ runs over all grid cells and $k$ over all polynomial degrees. 
Thus $\vec \phi_n$ is the vector of polynomial coefficients in cell $n$.
The operation $(\Eins\otimes \hat O) \phi$ then simply denotes that every 
coefficient vector is transformed by $\hat O $ in each cell. On the other
hand $(\hat U \otimes \Eins) \phi$ denotes that we linearly combine
(unchanged) vectors of different cells to get a new vector in cell $n$.
Actually the only operations we need in the first index is to get 
adjacent cells $n-1$ and $n+1$ so we define:
\begin{align}
    \Eins^{-}\phi_n := \phi_{n-1}\\
    \Eins^{+}\phi_n := \phi_{n+1}
    \label{eq:operator_one}
\end{align}
Some useful properties of the tensor product are
\begin{subequations}
    \begin{align}
        &\text{bilinearity }&A\otimes(\lambda B+ \mu C) &= \lambda (A\otimes B)+ \mu (A\otimes C) \\
        &&(\lambda A+ \mu B)\otimes A& = \lambda (A\otimes B)+ \mu (A\otimes C) \\
        &\text{associativity}& A\otimes (B\otimes C) &= (A\otimes B)\otimes C\\
        &\text{transposition}&(A\otimes B)^T &= A^T \otimes B^T \\
        &\text{inverse}&(A\otimes B)^{-1} &= A^{-1}\otimes B^{-1} \\
        &\text{product rule}& (A\otimes B)(C\otimes D) &= AC\otimes BD
        \label{eq:tensor_product_properties}
    \end{align}
\end{subequations}





\subsection{Matrix form of Poisson's equation}
We now derive the discretization matrix of the 1D Poisson equation for periodic
boundary conditions.
In equation (\ref{eq:dg_poisson}) we take $v=p_{ni}(x)$ and  $w=p_{nj}(x)$ 
and get
\begin{subequations}
    \begin{align}
        s_{jk} j_{nk} &=  (\phi_{nk}D_{kj} - \phi_{n+1,k}p_k(-1)p_j(1) + \phi_{nk}p_k(-1)p_j(-1)\\
        s_{ik} \rho_{nk} &= -j_{nk} D_{ki} + j_{nk}p_k(1)p_i(1) - j_{n-1,k}p_k(1)p_i(-1) \nonumber \\
         &+ [\phi_h](x_{n+1/2}) p_i(1) - [\phi_h](x_{n-1/2})p_i(-1) \\
        [\phi_h](x_{n+1/2}) &= \phi_{nk}p_k(1) - \phi_{n+1,k}p_k(-1)
        \label{ eq:projection}
    \end{align}
\end{subequations}
Now we use the previously introduced operator notation to enhance readability
\begin{subequations}
    \begin{align}
        (\Eins \otimes S)j =& [\Eins\otimes (D+L)^T - \Eins^{+} \otimes RL] \phi \\
        (\Eins \otimes S)\rho %=& [ -\Eins \otimes D^T + \Eins\otimes R - \Eins^{-} \otimes RL^T]j  \nonumber \\
                %+&[\Eins\otimes R - \Eins^{+}\otimes LR^T - \Eins^{-} \otimes RL^T + \Eins \otimes L]\phi\nonumber \\
                =& [\Eins\otimes (D+L) - \Eins^-\otimes LR]j \nonumber \\
                +& [\Eins\otimes (L+R) - \Eins^{-}\otimes LR  - \Eins^{+} \otimes RL] \phi
    \end{align}
    \label{eq:projection_operator_notation}
\end{subequations}
where we used $R-D^T = D+L$ and $LR^T = RL$. Now we insert $j$ into the equation
for $\rho$ and make use of bilinearity and the tensor product and inversion rules:
\begin{align}
    (\Eins\otimes S)\rho = 
    [\Eins&\otimes( (D+L)\circ T\circ (D+L)^T  L + R) \nonumber \\
            + \Eins^-\circ\Eins^+&\otimes (LR\circ T\circ RL) \nonumber \\
            - \Eins^+&\otimes ((D+L)\circ T \circ RL + RL) \nonumber \\
            - \Eins^-&\otimes (cLR\circ T\circ (D+L)^T + LR )]\phi
    \label{eq:laplace_operator}
\end{align}
From this form we can explicitely see that the resulting operator matrix is 
symmetric. (Note that $L^T = L$, $R^T = R$, $T^T = T$ and $\Eins^-\circ \Eins^+ = \Eins$ for
periodic boundaries). 
We name two independent matrices $A = (D+L)T(D+L)^T + L + R + LR T RL$ and $B = -\left( (D+L)T RL + RL \right)$ 
and remark the tridiagonal structure of the resulting matrix
\begin{align}
   M =  \begin{pmatrix}
        A&B& &B^T \\
        B^T&A&B& \\
         &B^T&A&B\\
        B& &B^T&A
    \end{pmatrix} 
    \label{eq:operator_matrix_periodic}
\end{align}
($\Eins^-$ and $\Eins^+$ have elements in the corners for periodic BC).
That means that we have to solve the equation 
\begin{align}
    \tilde \rho &= (\Eins\otimes S )\rho \nonumber \\
    M \phi &=  \tilde \rho 
    \label{eq:linear_system}
\end{align}
where $M$ is a symmetric positive definite matrix. We choose to solve 
this via a conjugate gradient method with $T(h)$ as a preconditioner
for several reasons. 
First and most importantly  
an iterative method seems better suited for parallel implementation than a
direct solver, because the computational expensive matrix-vector products 
can be parallelized. 
This is especially true since $M$ is sparse and 
has only two elements $A$ and $B$ which need to be stored. (i.e. the memory
requirement for $M$ is drastically reduced and the only 
thing we have to implement is a multiplication of $M$ times a dense vector. 
This operation however is SIMD par excellence)
The second reason is that we can use the information of previous timesteps 
to extrapolate the solution to (\ref{eq:linear_system}) in the 
current timestep.
This guess can then be used as the seed for the iteration and hopefully 
saves iterations and the need for a sophisticated preconditioner. 

We lastly remark that for peridic boundary conditions $M$ contains all 
the information on the discretization of the Laplace operator and can be used
both in the solution of Poissson's equation and in the solution of the 
diffusion equation. This might not be true for other type of boundary conditions
as we see in the next section.








\subsection{Proper treatment of boundary conditions}
In the case of Dirichlet or von Neumann boundary conditions we take $\Eins^+$ and
$\Eins^-$ to be strictly upper/lower triangular. Equations (\ref{eq:projection_operator_notation}) read
\begin{subequations}
    \begin{align}
        S\circ j_0 = c[ D^T\phi_0 - RL\phi_1] + \phi_b^0 \vec v_b^0 \\
        S\circ j_N = c[ D^T\phi_N + L\phi_N] - \phi_b^N \vec v_b^0 
    \end{align}
    \label{eq:dirichlet}
\end{subequations}
That means in matrix notation 
\begin{align}
    (\Eins\otimes S) j = c\begin{pmatrix}
        D^T & -RL & &  \\
          0 & (D^T + L) & -RL & \\
            &0& (D^T + L) & -RL \\
            & &0& (D^T + L)  
    \end{pmatrix}
    \phi + 
    \begin{pmatrix}
        \phi_b^0 \vec v_b^0 \\
        0\\
        0\\
        -\phi_b^N \vec v_b^N
    \end{pmatrix}
    \label{}
\end{align}
where $\vec v_{bj}^0 = (-1)^j$ and $\vec v_{bj}^N = 1$. $\phi_b^0$ is the 
left boundary value and $\phi_b^N$ is the right boundary value.
The equation for $\rho$ read:
\begin{align}
    \Eins\otimes S \rho = 
    \begin{pmatrix}
        D &0 & &\\
        -LR & D+L &0 & \\
            & -LR & D+L &0\\
            & & -LR & D+L
    \end{pmatrix}j+\nonumber \\
    \begin{pmatrix}
        L+R & -RL & &\\
        -LR & L+R & -RL&\\
            & -LR & L+R & -RL\\
            & & -LR & L+R
    \end{pmatrix}\phi +
    \begin{pmatrix}
        -\phi_b^0 \vec v_b^0 \\
        0\\
        0\\
        -\phi_b^N \vec v_b^N
    \end{pmatrix}
    \label{}
\end{align}
In total that means that we have to change the upper left 
matrix in $M$ to $cD\circ T \circ D^T + (L+R)$ and add the additional terms 
with $\phi_b$ to the right side. 

We only treat homogenous Neumann BC and state the result:
\begin{align}
    (\Eins\otimes S) j = c
    \begin{pmatrix}
        (D+L)^T & -RL & &  \\
          0 & (D + L)^T & -RL & \\
            &0& (D + L)^T & -RL \\
            & &0& -D  
    \end{pmatrix}
    \label{}
\end{align}
and 
\begin{align}
    (\Eins\otimes S) \rho =
    \begin{pmatrix}
        (D+L) &0 & &\\
        -LR & (D+L) &0 & \\
            & -LR & (D+L) &0\\
            & & -LR & -D^T
    \end{pmatrix}j+
    \begin{pmatrix}
        R & -RL & &\\
        -LR & (L+R) & -RL&\\
            & -LR & (L+R) & -RL\\
            & & -LR & L
    \end{pmatrix}\phi
    \label{}
\end{align}








\subsection{Numerical experiments}
We numerically confirm that the solution to $Mx=\overline \rho$ is, in the 
L2-norm, $O(h^P)$ away from the true solution to $\partial_x^2 \phi = \rho$.
This confirms the superconvergence property of the method. On an arbitrary grid
one could only have expected $O(h^{P-1/2})$ convergence.
We note however that for $P=1$ this gives suboptimal order since the classical 
$3$-point stencil has order $2$ and not $1$. (the penalty term costs one order)

With $100$ grid points we achieve approximately $10^{-6}$ accuracy for $P=3$ 
while we
need $1000$ grid points for $P=2$ to achieve the same accuracy and only 
$10$ grid points with $P=5$.

On the other side we also note the subtle fact of supraconvergence 
of the discretization. 
This means that if we apply the matrix $M$ to the discretization of $\sin(x)$ once
we cannot(!) expect the result to converge to $\sin(x)$ for vanishing gridsize $h$. 
Truncation error analysis shows that the scheme indeed has lower order or even is 
inconsistent for some $P$. This seems contradictory since we have an inconsistent
discretization on the one hand and on the other hand numerically showed 
convergence. We note that it is the solution $x$ of $Mx=\overline\rho$ that counts.  









\subsection{Extension to 2D}
In 2D we want to solve
\begin{align}
    c_1\partial_x^2 \phi + c_2 \partial_y^2 \phi = \rho
    \label{}
\end{align}
on $\Omega = [-a,a]\times[-b,b]$ with Dirichlet/Neumann BC in $x$ and periodic BC
in $y$. 
In the previous sections we managed to derive the DG - discretized version of
$c\partial_x^2$ for various BC of the heuristic form (valid for periodic BC)
$M = \Eins\otimes A + \Eins^+ \otimes B + \Eins^- B^T$ where $A$ is symmetric.
Now we want to construct $\Eins\otimes M  + M \otimes \Eins$. This form 
suggests an organisation of expansion coefficients $\phi_{nimj}$, i.e. the 
first two indices are $x$ and last two $y$ indices. 
We don't however
use this. 
We note that
the most natural way to store the expansion coefficients $\phi_h$ is in matrix
form $\phi_{nm}$ where each element consists of the Legendre coefficients $\phi_{nmij}$
in cell $I_n\times I_m$. This is justified because the number of Legendre 
coefficients is typically small in each dimension so $\phi_{nm}$ is e.g. a 
$3$ by $3$ matrix and access to both rows and columns is fast.
That means that we actually want to construct terms of the form
    $(\Eins_x\otimes \Eins_y) \otimes( \dots)$, 
    $(\Eins_x\otimes \Eins^-_y) \otimes( \dots)$, \dots, 
where $(\dots)$ stands for operators that work on the Legendre coefficients only.
This is achieved simply by constructing $\Eins\otimes M  + M \otimes \Eins$ 
(note that terms consist actually of four tensor products) and then swapping the 
second and third element in each term (which reflects the fact that we don't use
$\phi_{nimj}$ but $\phi_{nmij}$). This results in the (heuristic) form
\begin{align}
    M =&\ (\Eins_x\otimes\Eins_y) \otimes (\Eins\otimes A + A\otimes \Eins) \nonumber\\
     &+ (\Eins_x\otimes\Eins_y^-) \otimes( \Eins\otimes B^T) \nonumber \\
     &+ (\Eins_x\otimes\Eins_y^+) \otimes( \Eins\otimes B) \nonumber \\
     &+ (\Eins_x^-\otimes\Eins_y) \otimes( B^T \otimes \Eins) \nonumber \\
     &+ (\Eins_x^+\otimes\Eins_y) \otimes( B\otimes \Eins) 
    \label{}
\end{align}
which reflects the extremely local character of the method. Only coefficients from 
adjacent cells are needed to update one cell. We also note that still only $A$ and 
$B$ need to be stored to implement a matrix-vector multiplication with $M$.






\section{ The nonlinear Polarisation equation}

In one dimension the nonlinear Polarisation equation reads
\begin{align}
    -\partial_x ( c(x) \partial_x \phi) = \rho
    \label{}
\end{align}
The function $c(x)$ might be given in a DG expanded version $c(x) = c_lp_l(x)$.
We now discuss how to discretize this version in the framework of DG methods. 
We first rewrite the equation 
\begin{subequations}
    \begin{align}
      j + c\partial_x \phi &= 0\\
      \partial_x j &= \rho
        \label{}
    \end{align}
    \label{}
\end{subequations}
into three equations
\begin{subequations}
    \begin{align}
      \overline j + \partial_x \phi &= 0\\
      j = c\overline j\\
      \partial_x j &= \rho
        \label{}
    \end{align}
    \label{}
\end{subequations}
We note that the only new equation we have to discretize is the middle one.
We want to derive a discretization that 
reproduces the version for constant $c=1$ from the last section. 
We first introduce a useful notation for the product of two DG-expanded functions
in a given cell
\begin{align}
    S_{ij}(cg)_j = c_l g_k \int p_l p_k p_i =: c_l M_{lki} g_k =: C_{ik}g_k 
    \label{}
\end{align}
where $g(x)$ and $c(x)$ are arbitrary functions.
$M_{ijk}$ is symmetric in all of its indices and $C_{ik}$ is the 
contraction of $M$ with $c$ (and thus also symmetric). 
Note that the matrix $C = C_n$ 
depends on the cell index $n$. We thus write
\begin{align}
    j_{ni} = T_{ij}C_{njk}\overline j_{nk}
    \label{}
\end{align}
All in all, if we now insert the expression for $\overline j$ from the last section
we note that we have to simply replace every instance of $T$ by $TC^nT$ 
which is symmetric. Thus we have indeed preserved the symmetry of the matrix $M$ 
which is computational desirable of course. 

In order to get the limit of $c = c_0$ we note that $M_{0ij} = S_{ij}$ from where
we get $TC^nT = c_0T$ which is the correct limit.


In two dimensions we first introduce the function product 
\begin{align}
    \int a(x,y)b(x,y) p_k(x)p_l(y) dxdy = M_{kim}M_{ljn}a_{ij}b_{mn}
    \label{}
\end{align}
of two function $a(x,y) = a_{ij}p_i(x)p_j(y)$ and $b(x,y) = b_{mn}p_m(x)p_n(y)$.
We will write products of the form $M_{kim}M_{ljn}c_{ij}a_{mn} =: C_{kmln}a_{mn}$. 
Unfortunately we cannot write $C$ as the sum of two seperate operators
$\Eins\otimes C_x + C_y\otimes \Eins$. That means that we cannot decouple the
two-dimensional Poisson equation as in the last section. 
The generation of the global matrix $M$ will
thus be more complicated.











\end{document}
